{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Network to Detect Melanoma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Introduction\n",
    "In this notebook I will document my attempt to build a convolutional neural network to analyse images of lesions and then diagnose them. This project will take the image data from the [2017 ISIC Challenge on Skin Lesion Analysis Towards Melanoma Detection](https://challenge.kitware.com/#challenge/583f126bcad3a51cc66c8d9a).  The result will be one of three different skin diseases: melanoma, nevus, or seborrheic keratosis.\n",
    "\n",
    "The data was drawn from these three locations:\n",
    "\n",
    "1. [Training](https://s3-us-west-1.amazonaws.com/udacity-dlnfd/datasets/skin-cancer/train.zip)\n",
    "2. [Validation](https://s3-us-west-1.amazonaws.com/udacity-dlnfd/datasets/skin-cancer/valid.zip)\n",
    "3. [Testing](https://s3-us-west-1.amazonaws.com/udacity-dlnfd/datasets/skin-cancer/test.zip)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Plan\n",
    "Sebastian Thrun and his team of students found that a pretrained network [performed better](https://classroom.udacity.com/nanodegrees/nd101/parts/b9c4c3c3-b524-427b-8832-9d0748f14a2e/modules/cb574ac4-7144-4ba5-97b9-1c1265525ff8/lessons/54e18898-2666-445d-ba5c-ecab62a61d00/concepts/3e74481c-c1f3-45c1-b0b5-e1cf479df45d) than an untrained network. I intend to use transfer learning from one of the following pre-trained networks: VGG-19, ResNet-50, Inception, or Xception.\n",
    "\n",
    "I may explore other models if I have time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load Data?\n",
    "Maybe we will use this routine. The code below is using flow_from_directory, so perhaps not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from keras.applications import ResNet50\n",
    "from keras.applications import InceptionV3\n",
    "from keras.applications import Xception # TensorFlow ONLY\n",
    "from keras.applications import VGG16\n",
    "from keras.applications import VGG19\n",
    "from keras.applications import imagenet_utils\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [12.0, 10.0]\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.datasets import load_files\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "# define function to load train, test, and validation datasets\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    class_files = np.array(data['filenames'])\n",
    "    class_targets = np_utils.to_categorical(np.array(data['target']), 3)\n",
    "    return class_files, class_targets\n",
    "\n",
    "# load train, test, and validation datasets\n",
    "train_files, train_targets = load_dataset('data/train')\n",
    "valid_files, valid_targets = load_dataset('data/valid')\n",
    "test_files, test_targets = load_dataset('data/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Experimental code\n",
    "\n",
    "*Note: This code seems to be all over the internet, I forget which page I orignally copied it from, but you can find it [here](https://github.com/hbhasin/Image-Recognition-with-Deep-Learning/blob/master/classify_butterfly_image.py). I'm also borrowing code from this great resource on* [Transfer Learning using Keras](https://towardsdatascience.com/transfer-learning-using-keras-d804b2e04ef8).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading vgg19...\n"
     ]
    }
   ],
   "source": [
    "args = {'model': 'vgg19'\n",
    "\t, 'image':'donkey.jpg'}\n",
    "\n",
    "\n",
    "# define a dictionary that maps model names to their classes\n",
    "# inside Keras\n",
    "MODELS = {\n",
    "    \"vgg16\": VGG16,\n",
    "    \"vgg19\": VGG19,\n",
    "    \"inception\": InceptionV3,\n",
    "    \"xception\": Xception,  # TensorFlow ONLY\n",
    "    \"resnet\": ResNet50\n",
    "}\n",
    "## MY CODE ##\n",
    "FROZEN_LAYERS = {\n",
    "    \"vgg16\": 22,  # Does not Leaves final Conv2D block\n",
    "    \"vgg19\": 17,  #Leaves final Conv2D block\n",
    "    \"inception\": 311,   #Don't understand it, so freezing whole thing. Final classification layers are not imported anyway.\n",
    "    \"xception\": 132,  # TensorFlow ONLY, same note as for InceptionV3, don't understand it\n",
    "    \"resnet\": 175  #ditto don't understand it\n",
    "}\n",
    "\n",
    "# esnure a valid model name was supplied via command line argument\n",
    "if args[\"model\"] not in MODELS.keys():\n",
    "    raise AssertionError(\"The --model command line argument should \"\n",
    "                         \"be a key in the `MODELS` dictionary\")\n",
    "\n",
    "# initialize the input image shape (224x224 pixels) along with\n",
    "# the pre-processing function (this might need to be changed\n",
    "# based on which model we use to classify our image)\n",
    "inputShape = (224, 224)\n",
    "input_shape = (224, 224, 3)\n",
    "preprocess = imagenet_utils.preprocess_input\n",
    "\n",
    "# if we are using the InceptionV3 or Xception networks, then we\n",
    "# need to set the input shape to (299x299) [rather than (224x224)]\n",
    "# and use a different image processing function\n",
    "if args[\"model\"] in (\"inception\", \"xception\"):\n",
    "    inputShape = (299, 299)\n",
    "    input_shape = (299,299,3)\n",
    "    preprocess = preprocess_input\n",
    "\n",
    "# load our the network weights from disk (NOTE: if this is the\n",
    "# first time you are running this script for a given network, the\n",
    "# weights will need to be downloaded first -- depending on which\n",
    "# network you are using, the weights can be 90-575MB, so be\n",
    "# patient; the weights will be cached and subsequent runs of this\n",
    "# script will be *much* faster)\n",
    "print(\"[INFO] loading {}...\".format(args[\"model\"]))\n",
    "Network = MODELS[args[\"model\"]]\n",
    "model = Network(weights=\"imagenet\", include_top=False, input_shape = input_shape)\n",
    "inputShape\n",
    "## My Code\n",
    "#from keras import applications\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, Model \n",
    "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D\n",
    "#from keras import backend as k \n",
    "\n",
    "\n",
    "## FREEZE LAYERS\n",
    "for layer in model.layers[:FROZEN_LAYERS[args[\"model\"]]]:\n",
    "    layer.trainable = False\n",
    "\n",
    "## Define my classification layers\n",
    "#Adding custom Layers \n",
    "x = model.output\n",
    "x = Flatten()(x)\n",
    "#x = Dense(1024, activation=\"relu\")(x)\n",
    "#x = Dropout(0.5)(x)\n",
    "x = Dense(512, activation=\"relu\")(x)\n",
    "predictions = Dense(3, activation=\"softmax\")(x)\n",
    "\n",
    "# creating the final model \n",
    "model_final = Model(inputs = model.input, outputs = predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# load the input image using the Keras helper utility while ensuring\n",
    "# the image is resized to `inputShape`, the required input dimensions\n",
    "# for the ImageNet pre-trained network\n",
    "#print(\"[INFO] loading and pre-processing image...\")\n",
    "#image = load_img(args[\"image\"], target_size=inputShape)\n",
    "#image = img_to_array(image)\n",
    "\n",
    "# our input image is now represented as a NumPy array of shape\n",
    "# (inputShape[0], inputShape[1], 3) however we need to expand the\n",
    "# dimension by making the shape (1, inputShape[0], inputShape[1], 3)\n",
    "# so we can pass it through thenetwork\n",
    "#image = np.expand_dims(image, axis=0)\n",
    "\n",
    "# pre-process the image using the appropriate function based on the\n",
    "# model that has been loaded (i.e., mean subtraction, scaling, etc.)\n",
    "#image = preprocess(image)\n",
    "\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=inputShape)\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3) or (299, 299, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    img = np.expand_dims(x, axis=0)\n",
    "    return preprocess(img)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)\n",
    "\n",
    "# pre-process the data for Keras\n",
    "loaded = False\n",
    "try:\n",
    "    q = train_tensors[0]\n",
    "    loaded = True\n",
    "except:\n",
    "    pass\n",
    "if (loaded == False) | (last_inputShape != inputShape):\n",
    "    train_tensors = paths_to_tensor(train_files).astype('float32')\n",
    "    valid_tensors = paths_to_tensor(valid_files).astype('float32')\n",
    "    test_tensors = paths_to_tensor(test_files).astype('float32')\n",
    "    last_inputShape = inputShape\n",
    "\n",
    "\n",
    "# classify the image\n",
    "#print(\"[INFO] classifying image with '{}'...\".format(args[\"model\"]))\n",
    "#preds = model.predict(image)\n",
    "#P = imagenet_utils.decode_predictions(preds)\n",
    "\n",
    "# loop over the predictions and display the rank-5 predictions +\n",
    "# probabilities to our terminal\n",
    "#for (i, (imagenetID, label, prob)) in enumerate(P[0]):\n",
    "#    print(\"{}. {}: {:.2f}%\".format(i + 1, label, prob * 100))\n",
    "\n",
    "# load the image via OpenCV, draw the top prediction on the image,\n",
    "# and display the image to our screen\n",
    "#orig = cv2.imread(args[\"image\"])\n",
    "#(imagenetID, label, prob) = P[0][0]\n",
    "#cv2.putText(orig, \"Label: {}, {:.2f}%\".format(label, prob * 100),\n",
    "#\t(10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)\n",
    "#cv2.imshow(\"Classification\", orig)\n",
    "#cv2.waitKey(0)\n",
    "\n",
    "\n",
    "# convert BGR image to RGB for plotting\n",
    "#cv_rgb = cv2.cvtColor(orig, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# display the image, along with bounding box\n",
    "#plt.imshow(cv_rgb)\n",
    "#plt.title('Classy')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_3 False\n",
      "1 block1_conv1 False\n",
      "2 block1_conv2 False\n",
      "3 block1_pool False\n",
      "4 block2_conv1 False\n",
      "5 block2_conv2 False\n",
      "6 block2_pool False\n",
      "7 block3_conv1 False\n",
      "8 block3_conv2 False\n",
      "9 block3_conv3 False\n",
      "10 block3_conv4 False\n",
      "11 block3_pool False\n",
      "12 block4_conv1 False\n",
      "13 block4_conv2 False\n",
      "14 block4_conv3 False\n",
      "15 block4_conv4 False\n",
      "16 block4_pool False\n",
      "17 block5_conv1 True\n",
      "18 block5_conv2 True\n",
      "19 block5_conv3 True\n",
      "20 block5_conv4 True\n",
      "21 block5_pool True\n",
      "22 flatten_3 True\n",
      "23 dense_3 True\n",
      "24 dense_4 True\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               12845568  \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 1539      \n",
      "=================================================================\n",
      "Total params: 32,871,491.0\n",
      "Trainable params: 22,286,339.0\n",
      "Non-trainable params: 10,585,152.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i, layer in enumerate(model_final.layers):\n",
    "    print(i, layer.name, layer.trainable)\n",
    "model_final.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Another code snippet from the internet. Again, I've seen [this](https://towardsdatascience.com/transfer-learning-using-keras-d804b2e04ef8) is in more than one place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "150\n"
     ]
    }
   ],
   "source": [
    "train_data_dir = \"data/train\"\n",
    "validation_data_dir = \"data/valid\"\n",
    "batch_size = 40\n",
    "epochs = 50\n",
    "lr = 1e-3\n",
    "\n",
    "zoom_range = 0.3\n",
    "shift_range = 0.2\n",
    "rotation_range = 30\n",
    "\n",
    "import os\n",
    "def filecount(dir_name):\n",
    "# return the number of files in directory dir_name\n",
    "    try:\n",
    "        return len([f for f in os.listdir(dir_name) if os.path.isfile(os.path.join(dir_name, f))])\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "\n",
    "dirs = os.listdir(train_data_dir)\n",
    "nb_train_samples = 0\n",
    "for dir_name in dirs:\n",
    "    nb_train_samples += filecount(os.path.join(train_data_dir, dir_name))\n",
    "\n",
    "dirs = os.listdir(validation_data_dir)\n",
    "nb_validation_samples = 0\n",
    "for dir_name in dirs:\n",
    "    nb_validation_samples += filecount(os.path.join(validation_data_dir, dir_name))\n",
    "\n",
    "print(nb_train_samples)\n",
    "print(nb_validation_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(224, 224, 3)\n",
      "(3,)\n"
     ]
    }
   ],
   "source": [
    "print(train_tensors[1].shape)\n",
    "print(train_targets[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 3 classes.\n",
      "Found 150 images belonging to 3 classes.\n",
      "Train on 2000 samples, validate on 150 samples\n",
      "Epoch 1/50\n",
      "Epoch 00000: val_acc improved from -inf to 0.52000, saving model to saved_models/vgg19_1.hdf5\n",
      "21s - loss: 5.0611 - acc: 0.6860 - val_loss: 7.7367 - val_acc: 0.5200\n",
      "Epoch 2/50\n",
      "Epoch 00001: val_acc did not improve\n",
      "20s - loss: 5.0611 - acc: 0.6860 - val_loss: 7.7367 - val_acc: 0.5200\n",
      "Epoch 3/50\n",
      "Epoch 00002: val_acc did not improve\n",
      "19s - loss: 5.0611 - acc: 0.6860 - val_loss: 7.7367 - val_acc: 0.5200\n",
      "Epoch 4/50\n",
      "Epoch 00003: val_acc did not improve\n",
      "20s - loss: 5.0611 - acc: 0.6860 - val_loss: 7.7367 - val_acc: 0.5200\n",
      "Epoch 5/50\n",
      "Epoch 00004: val_acc did not improve\n",
      "20s - loss: 5.0611 - acc: 0.6860 - val_loss: 7.7367 - val_acc: 0.5200\n",
      "Epoch 6/50\n",
      "Epoch 00005: val_acc did not improve\n",
      "20s - loss: 5.0611 - acc: 0.6860 - val_loss: 7.7367 - val_acc: 0.5200\n",
      "Epoch 7/50\n",
      "Epoch 00006: val_acc did not improve\n",
      "20s - loss: 5.0611 - acc: 0.6860 - val_loss: 7.7367 - val_acc: 0.5200\n",
      "Epoch 8/50\n",
      "Epoch 00007: val_acc did not improve\n",
      "20s - loss: 5.0611 - acc: 0.6860 - val_loss: 7.7367 - val_acc: 0.5200\n",
      "Epoch 9/50\n",
      "Epoch 00008: val_acc did not improve\n",
      "20s - loss: 5.0611 - acc: 0.6860 - val_loss: 7.7367 - val_acc: 0.5200\n",
      "Epoch 10/50\n",
      "Epoch 00009: val_acc did not improve\n",
      "20s - loss: 5.0611 - acc: 0.6860 - val_loss: 7.7367 - val_acc: 0.5200\n",
      "Epoch 11/50\n",
      "Epoch 00010: val_acc did not improve\n",
      "20s - loss: 5.0611 - acc: 0.6860 - val_loss: 7.7367 - val_acc: 0.5200\n",
      "Epoch 12/50\n",
      "Epoch 00011: val_acc did not improve\n",
      "20s - loss: 5.0611 - acc: 0.6860 - val_loss: 7.7367 - val_acc: 0.5200\n",
      "Epoch 00011: early stopping\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping\n",
    "# compile the model\n",
    "opt = optimizers.RMSprop(lr=lr)\n",
    "model_final.compile(loss = \"categorical_crossentropy\", optimizer = opt, metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "# Initiate the train and test generators with data Augumentation \n",
    "train_datagen = ImageDataGenerator(\n",
    "rescale = 1./inputShape[0],\n",
    "horizontal_flip = True,\n",
    "fill_mode = \"nearest\",\n",
    "zoom_range = zoom_range,\n",
    "width_shift_range  = shift_range,\n",
    "height_shift_range = shift_range,\n",
    "rotation_range     = rotation_range)\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "rescale = 1./inputShape[0],\n",
    "horizontal_flip = True,\n",
    "fill_mode = \"nearest\",\n",
    "zoom_range = zoom_range,\n",
    "width_shift_range  = shift_range,\n",
    "height_shift_range = shift_range,\n",
    "rotation_range     = rotation_range)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "train_data_dir,\n",
    "target_size = inputShape,\n",
    "batch_size = batch_size, \n",
    "class_mode = \"categorical\")\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "validation_data_dir,\n",
    "target_size = inputShape,\n",
    "class_mode = \"categorical\")\n",
    "\n",
    "# Save the model according to the conditions\n",
    "modelCheckpointName = 'saved_models/' + args[\"model\"] + '_1.hdf5'\n",
    "checkpoint = ModelCheckpoint(modelCheckpointName, monitor='val_acc', verbose=1, save_best_only=True,\n",
    "                             save_weights_only=False, mode='auto', period=1)\n",
    "early = EarlyStopping(monitor='val_acc', min_delta=0, patience=10, verbose=1, mode='auto')\n",
    "\n",
    "\n",
    "# Train the model \n",
    "#hist = model_final.fit_generator(train_generator,\n",
    "#    steps_per_epoch = nb_train_samples,     epochs = epochs,\n",
    "#    validation_data = validation_generator, validation_steps = nb_validation_samples,\n",
    "#    callbacks = [checkpoint, early])\n",
    "\n",
    "hist = model_final.fit(train_tensors, train_targets, \n",
    "          validation_data=(valid_tensors, valid_targets),\n",
    "          epochs=epochs, batch_size=batch_size, callbacks=[checkpoint, early], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAArwAAAJQCAYAAABsPLe6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAEDTBC8AAE0TvAAANE3wAgDQtP8Pes8fe3VMjowAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x26beb279828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ['acc', 'loss', 'val_acc', 'val_loss']\n",
    "yl = hist.history['loss']\n",
    "ya = hist.history['acc']\n",
    "yvl = hist.history['val_loss']\n",
    "yva = hist.history['val_acc']\n",
    "x = np.arange(len(yl))\n",
    "\n",
    "plt.plot(x,yl, label='Loss')\n",
    "plt.plot(x,ya, label='Accuracy')\n",
    "plt.plot(x,yvl, label='Val Loss')\n",
    "plt.plot(x,yva, label='Val Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "#plt.ylabel('Loss/Accuracy')\n",
    "plt.legend(loc=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best weights back into model...\n",
      "vgg19 Test accuracy: 65.5000%\n"
     ]
    }
   ],
   "source": [
    "#Load the model weights with the best validation loss.\n",
    "print('Loading best weights back into model...')\n",
    "model_final.load_weights(modelCheckpointName)\n",
    "\n",
    "# Calculate classification accuracy on the test dataset.\n",
    "# get index of predicted class for each image in test set\n",
    "class_predictions = [np.argmax(model_final.predict(np.expand_dims(feature, axis=0)))\\\n",
    "                           for feature in test_tensors]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(class_predictions)==np.argmax(test_targets, axis=1))/len(class_predictions)\n",
    "print(args[\"model\"] + ' Test accuracy: %.4f%%' % test_accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
