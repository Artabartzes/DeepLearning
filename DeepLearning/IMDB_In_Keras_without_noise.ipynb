{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing IMDB Data in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import optimizers\n",
    "from keras.backend import eval\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the data\n",
    "This dataset comes preloaded with Keras, so one simple command will get us training and testing data. There is a parameter for how many words we want to look at. We've set it at 1000, but feel free to experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,)\n",
      "(25000,)\n"
     ]
    }
   ],
   "source": [
    "# Loading the data (it's preloaded in Keras)\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=1000)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. A. Combine test and training data so we can filter out noise\n",
    "Use log polarization metric from Trask's Sentiment NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "998\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "positive_counts = Counter()\n",
    "negative_counts = Counter()\n",
    "total_counts = Counter()\n",
    "\n",
    "x_total = np.concatenate((x_train,x_test))\n",
    "y_total = np.concatenate((y_train,y_test))\n",
    "for idx, review in np.ndenumerate(x_total):\n",
    "    for word in review:\n",
    "        if y_total[idx[0]] == 0:\n",
    "            positive_counts[word] +=1\n",
    "            total_counts[word] +=1\n",
    "        elif y_total[idx[0]] == 1:\n",
    "            negative_counts[word] +=1\n",
    "            total_counts[word] +=1\n",
    "\n",
    "# Build Ratios\n",
    "min_count = 10\n",
    "pos_neg_ratios = Counter()\n",
    "\n",
    "for word, count in list(total_counts.most_common()):\n",
    "    if count > min_count:\n",
    "        ratio = positive_counts[word] / float(negative_counts[word]+1)\n",
    "        if ratio > 1:\n",
    "            pos_neg_ratios[word] = np.log(ratio)\n",
    "        else:\n",
    "            pos_neg_ratios[word] = -np.log((1 / (ratio + 0.01)))\n",
    "\n",
    "print(len(pos_neg_ratios))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. B. Distribution of Ratios\n",
    "Now, examine the distribution of ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x173b50a7fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(list(pos_neg_ratios.values()), bins=60)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's cutoff the words with an absolute value of <= 1 STD (STD = 0.47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polarity Cutoff =  0.235159119556\n",
      "Sample Review before:  [1, 5, 14, 9, 6, 55, 2, 22, 13, 203, 30, 355, 21, 14, 9, 4, 236, 22, 121, 13, 2, 2, 2, 35, 779, 284, 37, 2, 4, 217, 5, 2, 6, 749, 10, 10, 2, 2, 5, 2, 2, 26, 82, 321, 36, 26, 2, 5, 2, 2, 2, 8, 358, 4, 704, 117, 122, 36, 124, 51, 62, 593, 375, 10, 10, 4, 2, 5, 732, 26, 821, 5, 2, 14, 16, 159, 4, 504, 7, 2, 2, 10, 10, 51, 9, 91, 2, 44, 14, 22, 9, 4, 192, 15, 2, 40, 14, 131, 2, 11, 938, 704, 2, 131, 2, 543, 84, 12, 9, 220, 6, 2, 5, 6, 320, 237, 4, 2, 325, 10, 10, 25, 80, 358, 14, 22, 12, 16, 814, 11, 4, 2, 2, 7, 2, 2, 63, 131, 2, 43, 92, 2, 501, 15, 8, 2, 2, 15, 2, 131, 47, 24, 77, 2, 237, 2, 2, 158, 158]\n",
      "Old maximum review length = 2494\n",
      "New maximum review length = 353\n",
      "Sample Review after:  [55, 203, 355, 779, 217, 82, 321, 36, 358, 704, 36, 62, 821, 40, 131, 938, 704, 131, 543, 320, 325, 80, 358, 131, 43, 92, 501, 131]\n"
     ]
    }
   ],
   "source": [
    "polarity_cutoff = np.std(list(pos_neg_ratios.values())) * .75\n",
    "print('Polarity Cutoff = ', polarity_cutoff)\n",
    "max_review = 0\n",
    "max_new = 0\n",
    "print('Sample Review before: ',x_total[150])\n",
    "for idx, review in np.ndenumerate(x_total):\n",
    "    max_review = max(max_review,len(review))\n",
    "    new_review = []\n",
    "    for word in review:\n",
    "        if np.abs(pos_neg_ratios[word]) > polarity_cutoff:\n",
    "            new_review.append(word)\n",
    "    x_total[idx] = new_review\n",
    "    max_new = max(max_new,len(new_review))\n",
    "print('Old maximum review length =', max_review)\n",
    "print('New maximum review length =', max_new)\n",
    "print('Sample Review after: ',x_total[150])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length= 353\n"
     ]
    }
   ],
   "source": [
    "xmax = 0\n",
    "for x in x_total:\n",
    "    xmax = max(xmax,len(x))\n",
    "print('Maximum length=',xmax)\n",
    "num_words = xmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split it up again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_total, y_total, test_size = 0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Examining the data\n",
    "Notice that the data has been already pre-processed, where all the words have numbers, and the reviews come in as a vector with the words that the review contains. For example, if the word 'the' is the first one in our dictionary, and a review contains the word 'the', then there is a 1 in the corresponding vector.\n",
    "\n",
    "The output comes as a vector of 1's and 0's, where 1 is a positive sentiment for the review, and 0 is negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[308, 321, 488, 20, 403, 92, 128, 100, 20, 488, 55, 406, 48, 897, 217, 988, 670, 87, 257, 257, 696, 389, 262, 922, 81, 403, 299, 452, 153, 100, 20]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. One-hot encoding the output\n",
    "Here, we'll turn the input vectors into (0,1)-vectors. For example, if the pre-processed vector contains the number 14, then in the processed vector, the 14th entry will be 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.\n",
      "  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "# One-hot encoding the output into vector mode, each of length 1000\n",
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "x_train = tokenizer.sequences_to_matrix(x_train, mode='binary')\n",
    "x_test = tokenizer.sequences_to_matrix(x_test, mode='binary')\n",
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we'll also one-hot encode the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 353) (25000, 2)\n",
      "(25000, 353) (25000, 2)\n"
     ]
    }
   ],
   "source": [
    "# One-hot encoding the output\n",
    "num_classes = 2\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "print(x_train.shape,y_train.shape)\n",
    "print(x_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Building the  model architecture\n",
    "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "353\n",
      "(25000, 353)\n"
     ]
    }
   ],
   "source": [
    "print(num_words)\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 353)               124962    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 353)               124962    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 708       \n",
      "=================================================================\n",
      "Total params: 250,632\n",
      "Trainable params: 250,632\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# TODO: Build the model architecture\n",
    "# Building the model\n",
    "model = Sequential()\n",
    "model.add(Dense(int(num_words), activation='tanh', input_dim=num_words))\n",
    "#model.add(Dropout(.2))\n",
    "model.add(Dense(num_words, activation='tanh'))\n",
    "#model.add(Dropout(.2))\n",
    "#model.add(Dense(num_words, activation='relu'))\n",
    "#model.add(Dropout(.1))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# TODO: Compile the model using a loss function and an optimizer.\n",
    "#sgd = optimizers.SGD(lr=1e-5, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "opt = optimizers.RMSprop(lr=1e-4)\n",
    "#model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training the model\n",
    "Run the model here. Experiment with different batch_size, and number of epochs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "25000/25000 [==============================] - 5s - loss: 0.4845 - acc: 0.7656     \n",
      "Epoch 2/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.4572 - acc: 0.7829     \n",
      "Epoch 3/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.4570 - acc: 0.7852     \n",
      "Epoch 4/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.4563 - acc: 0.7858     \n",
      "Epoch 5/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.4556 - acc: 0.7859     \n",
      "Epoch 6/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.4550 - acc: 0.7865     \n",
      "Epoch 7/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.4542 - acc: 0.7868     \n",
      "Epoch 8/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.4537 - acc: 0.7865     \n",
      "Epoch 9/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.4528 - acc: 0.7872     \n",
      "Epoch 10/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.4521 - acc: 0.7880     \n",
      "Epoch 11/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.4512 - acc: 0.7868     \n",
      "Epoch 12/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.4502 - acc: 0.7884     \n",
      "Epoch 13/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.4493 - acc: 0.7877     \n",
      "Epoch 14/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.4486 - acc: 0.7896     \n",
      "Epoch 15/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.4473 - acc: 0.7883     \n",
      "Epoch 16/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.4470 - acc: 0.7895     \n",
      "Epoch 17/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.4456 - acc: 0.7904     \n",
      "Epoch 18/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.4449 - acc: 0.7916     \n",
      "Epoch 19/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.4439 - acc: 0.7922     \n",
      "Epoch 20/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.4432 - acc: 0.7918     \n",
      "Epoch 21/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.4427 - acc: 0.7920     \n",
      "Epoch 22/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.4414 - acc: 0.7925     \n",
      "Epoch 23/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.4405 - acc: 0.7932     \n",
      "Epoch 24/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.4396 - acc: 0.7944     \n",
      "Epoch 25/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.4382 - acc: 0.7944     \n",
      "Epoch 26/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.4369 - acc: 0.7957     \n",
      "Epoch 27/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.4364 - acc: 0.7963     \n",
      "Epoch 28/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.4351 - acc: 0.7956     \n",
      "Epoch 29/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.4338 - acc: 0.7974     \n",
      "Epoch 30/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.4319 - acc: 0.7972     \n",
      "Epoch 31/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.4306 - acc: 0.8000     \n",
      "Epoch 32/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.4291 - acc: 0.8004     \n",
      "Epoch 33/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.4276 - acc: 0.7998     \n",
      "Epoch 34/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.4259 - acc: 0.8020     \n",
      "Epoch 35/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.4247 - acc: 0.8020     \n",
      "Epoch 36/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.4230 - acc: 0.8038     \n",
      "Epoch 37/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.4210 - acc: 0.8045     \n",
      "Epoch 38/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.4194 - acc: 0.8066     \n",
      "Epoch 39/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.4178 - acc: 0.8055     \n",
      "Epoch 40/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.4159 - acc: 0.8076     \n",
      "Epoch 41/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.4142 - acc: 0.8099     \n",
      "Epoch 42/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.4121 - acc: 0.8114     \n",
      "Epoch 43/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.4107 - acc: 0.8096     \n",
      "Epoch 44/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.4086 - acc: 0.8124     \n",
      "Epoch 45/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.4067 - acc: 0.8137     \n",
      "Epoch 46/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.4048 - acc: 0.8131     \n",
      "Epoch 47/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.4031 - acc: 0.8154     \n",
      "Epoch 48/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.4009 - acc: 0.8171     \n",
      "Epoch 49/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.3992 - acc: 0.8165     \n",
      "Epoch 50/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.3971 - acc: 0.8180     \n",
      "Epoch 51/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.3954 - acc: 0.8196     \n",
      "Epoch 52/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.3933 - acc: 0.8215     \n",
      "Epoch 53/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.3913 - acc: 0.8223     \n",
      "Epoch 54/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.3893 - acc: 0.8243     \n",
      "Epoch 55/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.3874 - acc: 0.8236     \n",
      "Epoch 56/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.3853 - acc: 0.8272     \n",
      "Epoch 57/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.3833 - acc: 0.8262     \n",
      "Epoch 58/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.3811 - acc: 0.8278     \n",
      "Epoch 59/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.3788 - acc: 0.8284     \n",
      "Epoch 60/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.3768 - acc: 0.8310     \n",
      "Epoch 61/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.3739 - acc: 0.8315     \n",
      "Epoch 62/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.3723 - acc: 0.8314     \n",
      "Epoch 63/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.3697 - acc: 0.8323     \n",
      "Epoch 64/150\n",
      "25000/25000 [==============================] - 4s - loss: 0.3676 - acc: 0.8350     \n",
      "Epoch 65/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.3653 - acc: 0.8369     \n",
      "Epoch 66/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.3626 - acc: 0.8372     \n",
      "Epoch 67/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.3603 - acc: 0.8384     \n",
      "Epoch 68/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.3577 - acc: 0.8429     \n",
      "Epoch 69/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.3548 - acc: 0.8417     \n",
      "Epoch 70/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.3527 - acc: 0.8429     \n",
      "Epoch 71/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.3502 - acc: 0.8457     \n",
      "Epoch 72/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.3471 - acc: 0.8466     \n",
      "Epoch 73/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.3445 - acc: 0.8479     \n",
      "Epoch 74/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.3419 - acc: 0.8495     \n",
      "Epoch 75/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.3387 - acc: 0.8521     \n",
      "Epoch 76/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.3363 - acc: 0.8526     \n",
      "Epoch 77/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.3334 - acc: 0.8530     \n",
      "Epoch 78/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.3300 - acc: 0.8562     \n",
      "Epoch 79/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.3276 - acc: 0.8589     \n",
      "Epoch 80/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.3239 - acc: 0.8599     \n",
      "Epoch 81/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.3215 - acc: 0.8609     \n",
      "Epoch 82/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.3182 - acc: 0.8627     \n",
      "Epoch 83/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.3148 - acc: 0.8673     \n",
      "Epoch 84/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.3118 - acc: 0.8667     \n",
      "Epoch 85/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.3089 - acc: 0.8692     \n",
      "Epoch 86/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.3059 - acc: 0.8712     \n",
      "Epoch 87/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.3024 - acc: 0.8725     \n",
      "Epoch 88/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.2993 - acc: 0.8730     \n",
      "Epoch 89/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.2959 - acc: 0.8768     \n",
      "Epoch 90/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.2927 - acc: 0.8786     \n",
      "Epoch 91/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.2890 - acc: 0.8812     \n",
      "Epoch 92/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.2858 - acc: 0.8843     \n",
      "Epoch 93/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.2819 - acc: 0.8838     \n",
      "Epoch 94/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.2786 - acc: 0.8873     \n",
      "Epoch 95/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.2752 - acc: 0.8895     \n",
      "Epoch 96/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.2711 - acc: 0.8922     \n",
      "Epoch 97/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.2678 - acc: 0.8929     \n",
      "Epoch 98/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.2646 - acc: 0.8962     \n",
      "Epoch 99/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.2610 - acc: 0.8976     \n",
      "Epoch 100/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.2574 - acc: 0.8996     \n",
      "Epoch 101/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.2539 - acc: 0.9014     \n",
      "Epoch 102/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.2496 - acc: 0.9042     \n",
      "Epoch 103/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.2464 - acc: 0.9062     \n",
      "Epoch 104/150\n",
      "25000/25000 [==============================] - 25s - loss: 0.2425 - acc: 0.9084    \n",
      "Epoch 105/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.2383 - acc: 0.9110     \n",
      "Epoch 106/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.2349 - acc: 0.9120     \n",
      "Epoch 107/150\n",
      "25000/25000 [==============================] - 4s - loss: 0.2308 - acc: 0.9142     \n",
      "Epoch 108/150\n",
      "25000/25000 [==============================] - 4s - loss: 0.2267 - acc: 0.9171     \n",
      "Epoch 109/150\n",
      "25000/25000 [==============================] - 4s - loss: 0.2234 - acc: 0.9188     \n",
      "Epoch 110/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.2191 - acc: 0.9208     \n",
      "Epoch 111/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.2156 - acc: 0.9236     \n",
      "Epoch 112/150\n",
      "25000/25000 [==============================] - 4s - loss: 0.2115 - acc: 0.9260     \n",
      "Epoch 113/150\n",
      "25000/25000 [==============================] - 4s - loss: 0.2076 - acc: 0.9282     \n",
      "Epoch 114/150\n",
      "25000/25000 [==============================] - 4s - loss: 0.2035 - acc: 0.9299     \n",
      "Epoch 115/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.1995 - acc: 0.9327     \n",
      "Epoch 116/150\n",
      "25000/25000 [==============================] - 4s - loss: 0.1959 - acc: 0.9352     \n",
      "Epoch 117/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.1915 - acc: 0.9360     \n",
      "Epoch 118/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.1877 - acc: 0.9381     \n",
      "Epoch 119/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.1840 - acc: 0.9394     \n",
      "Epoch 120/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.1797 - acc: 0.9430     \n",
      "Epoch 121/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.1761 - acc: 0.9442     \n",
      "Epoch 122/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.1723 - acc: 0.9455     \n",
      "Epoch 123/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.1684 - acc: 0.9471     \n",
      "Epoch 124/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.1642 - acc: 0.9502     \n",
      "Epoch 125/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.1603 - acc: 0.9522     \n",
      "Epoch 126/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.1570 - acc: 0.9532     \n",
      "Epoch 127/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.1529 - acc: 0.9556     \n",
      "Epoch 128/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.1489 - acc: 0.9586     \n",
      "Epoch 129/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.1449 - acc: 0.9595     \n",
      "Epoch 130/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.1419 - acc: 0.9614     \n",
      "Epoch 131/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.1380 - acc: 0.9626     \n",
      "Epoch 132/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.1344 - acc: 0.9638     \n",
      "Epoch 133/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.1306 - acc: 0.9657     \n",
      "Epoch 134/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.1272 - acc: 0.9672     \n",
      "Epoch 135/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.1235 - acc: 0.9692     \n",
      "Epoch 136/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.1200 - acc: 0.9708     \n",
      "Epoch 137/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.1161 - acc: 0.9720     \n",
      "Epoch 138/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.1133 - acc: 0.9727     \n",
      "Epoch 139/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.1098 - acc: 0.9741     \n",
      "Epoch 140/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.1061 - acc: 0.9762     \n",
      "Epoch 141/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.1029 - acc: 0.9772     \n",
      "Epoch 142/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.0996 - acc: 0.9782     \n",
      "Epoch 143/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.0964 - acc: 0.9793     \n",
      "Epoch 144/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.0932 - acc: 0.9803     \n",
      "Epoch 145/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.0902 - acc: 0.9819     \n",
      "Epoch 146/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.0873 - acc: 0.9830     \n",
      "Epoch 147/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.0845 - acc: 0.9837     \n",
      "Epoch 148/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.0815 - acc: 0.9846     \n",
      "Epoch 149/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.0782 - acc: 0.9859     \n",
      "Epoch 150/150\n",
      "25000/25000 [==============================] - 3s - loss: 0.0757 - acc: 0.9866     \n"
     ]
    }
   ],
   "source": [
    "# TODO: Run the model. Feel free to experiment with different batch sizes and number of epochs.\n",
    "epochs = 150\n",
    "batch_size = 50\n",
    "\n",
    "hist = model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Historical Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3Xl8XXd95//XR/u+S160WN5jx46X\nOHZ2shIHQkKAAkmAQDOTKQMD0ynMwJRO++tvlk5/7XT5wTBNgRJIIUBIIKQhkKRkK7HjJV7iJd5t\nSZZt2dZm2VrvZ/74HsmKI9s3jq7ulfV+Ph73ce8599yrj46t8znf3dwdERERgLRkByAiIqlDSUFE\nRIYoKYiIyBAlBRERGaKkICIiQ5QURERkiJKCiIgMUVIQEZEhSgoiIjIkI9kBvFMVFRVeX1+f7DBE\nRMaVdevWHXX3yvMdN+6SQn19PWvXrk12GCIi44qZ7Y/nOFUfiYjIECUFEREZkrCkYGbfMbMjZvbG\nWd43M/tbM9tlZpvMbGmiYhERkfgksqTwXWDlOd6/HZgdPR4EvpnAWEREJA4JSwru/hJw/ByH3AV8\nz4NVQImZTUlUPCIicn7JbFOoBhqGbTdG+97GzB40s7VmtralpWVMghMRmYiSmRRshH0jLgPn7g+5\n+zJ3X1ZZed5utiIicoGSOU6hEagdtl0DHExSLCIiqSM2ACePQet+aNsPXUfhVCvMeS9UX57QH53M\npPAk8HkzexRYAbS7e3MS4xERGRs9ndDeCMd2w/Hd4bl1L5xogZNHQ0Lw2Ns/V1A1fpOCmf0QuAGo\nMLNG4I+BTAB3/z/A08D7gF3ASeAziYpFRGRMtTfBka3Q0QQdB4c9R4+ejrcen1cOZTOgfCbUrYD8\nyvAomQal0yC/CnJLIC094aEnLCm4+z3ned+BzyXq54uIJIx7uLgf2xUu+CePQUczHNsJh7eEfUMM\nCidD0VSomA0zbgivi6qhbDqUzQwX/BQx7uY+EhEZU13H4NBGOLQZmjdBy5uhyqfv5FuPy8wLF/i6\nq6DmCpiyCEpqoWASpGcmJ/YLoKQgItLfC+0N0LoP2g6Ext0j20IS6BzW/6W4FqrmwfTrQ1VP+axw\n4c+rgOxCsJE6VY4vSgoiMrF0HoZdz4aePZ3NcPgNOPQGxPpOH5OWGS7406+DyQth8mXhOa8seXGP\nESUFEbl49Z4Mdf9t++DAatj7IjS8BjhYWrjDr5wLV/1bqLzkdMNu4ZQxadRNRUoKInLx6OuG3f8M\nbz4NjWuhZTtDY2ItPdTz3/BVuOT9oRpogl74z0VJQUTGn5PH39rHf/D56E7o64KcYqi9EubfFbp6\nFleHKqCcomRHnvKUFEQk9fV1h0bghlWw4QfQsPr0e5YWGoDLZ8KST4RRv9PfM656/KQSJQURSR2x\n2OnunyePhURwYPVbq4Eq5sJNX4NJC0MiKJkGGVnJjPqioqQgIskz0A+HNsGBVXDgVdj3CpwaNuN+\ndjHUXgHz74Ty2VB1CUxacFF0/UxVSgoiMnYG+kLVz75XQhJoWBPaAABK6mDObTDjRqhdHub5ycxT\nAhhjSgoikjixWGgE3vtS6BW050Xo7QQs3PEvuQ/qrgyNwsUjLqciY0xJQURGz6k22P18KAE0bwxV\nQ70nwnvFdbDwIzDrFqi/NqXm+5HTlBRE5MKcagvTQbQ3QtP60C7QsApi/ZCRG0YAL7onjA2oXREm\ng1NVUMpTUhCR+HS3w/5XQ1XQ3pfg8ObT71l6SAJXfT4MDKu+XAPDxiklBRF5O/dQCjiwOjQMN7wG\nR7aEhV/Ss8Oc/zd+LfQGKpoapojIyk921DIKlBREJFQFHd0RLv4Nq8LzicPhvaxCqFkG1385tAXU\nLIfMnOTGKwmjpCAy0fR1h7EAR3fA5sdg569PJwCA0vqwEEzt8tAWUDVfVUETiJKCyMWuux32/za0\nA+x5MVQDDcoqgDkrYcplYXBY9eVQOCl5sUrSKSmIXEyO7w1LRGbmhl5BGx8NycAHICMnjAmY95Ww\nPGThlLBYTFZesqOWFKKkIDKexQagaV2YKvrNZ6Bl21vfL6mDa74IM28K1UEZ2cmJU8YNJQWR8aZ1\nH6x7OCwYc3gr9J8KXULrr4GlnwpVQP3doWpo6hJIS0t2xDKOKCmIpKKB/jAz6MH10ULxe6GrJbQP\nHN0RBoHVXQ3LPhOSwKybIbc02VHLRUBJQSTZOpph7bfDBT+7CI5shb0vR3MEEdoCSqeHBuDCybDg\nQ7Dkk5orSBJCSUFkLA30he6f7U3h4t+4JnQLjfVDdgF0d4R2gIUfhmnXwNSlYeUwVQHJGFFSEEmU\nWAya1oZxAHteDCOETxxhaLEYCCWDRR+Da/8DlE0PI4k1P5AkkZKCyLvVeThc/Ls7woygvV3QeQi2\n/QI6GsNykTVXwOz3hikhCqdEU0PMDauGDU8CSgiSZEoKIvFwD4vFtzeE/v+t++DINmjeAIffePvx\naZmhG+gtfwyzb1UjsIwbSgoig9qbQo+f4towoKutIawJsOOZMClc38m3Hp9fCZMuhZv/GOqvg/yK\n0A00Kz8MHtNdv4xDSgoy8biHC3x/z+nF4bc8AZt+FBp8z1QxF5Z8IvQAKq4Jj5K6kARELjJKCnLx\ni8Xg4Ouw67mwLvDB9aH753AZubDsAZh3R2gP6D0RVgqrmA2l05ITt0gSKCnI+NXfC6da4eTRsOrX\n3hdDn/+eznDHbxZed7VEJYBoXeD5Hww9fTJyw5KQpfVhPQAtDymipCApquto6MefkROmcNj2ZOjN\nk54Vqm06DkLr3rDoy6CiGqiYBQVVkJ4ZqomyCyC/Kkz/PPMmyC9P3u8kMg4oKUjyxGKhN8+RraF6\n5/CWMLCrozl05RzO0mHmjSFJdB2FyQvCyN7CyZBTEtYBLp+lxl2Rd0lJQRInNhAu9I1rQrfN9qZw\n547BsZ1wdOfpHj2WFubzL5oSJnabtCCs+RsbCPX7064OJQARSaiEJgUzWwn8DZAOfMvd/+yM9+uA\nh4GS6JivuPvTiYxJ3qWTx8Od/dEdoXpm8sIwI+eOX4WL//G9oRE3pzjs7+kIn8suhtI6OHYy1O+X\nz4LLrw0DuCovCXf+WuNXJOkSlhTMLB34BnAr0AisMbMn3X3rsMO+BvzY3b9pZvOBp4H6RMUkcYjF\n4Oib4QIPoc/+icOh186B1W+fr39QehZUL4M5t0FeWUgMlh4WdaldEbpwqmpHJOUlsqSwHNjl7nsA\nzOxR4C5geFJwoCh6XQwcTGA8E1Ns4HSf/MxcSM8OVTmNa8J+dzi+Gw69AZ3Nod/+QO/bvye7OCzS\nsvAjYY7+yrmhkffQ5lD1U39dVDUkIuNZIpNCNdAwbLsRWHHGMX8C/NrM/h2QD9ySwHguHrFY6IZ5\nqi0ssJJTElbUOrwlVOsA9J2CPS+EtXljfef+vtyyUH0z6WbIKw/VOTXLQw+etgNhioaqeSMv3l5S\nN+q/nogkTyKTwkh1BX7G9j3Ad939L83sKuD7ZrbAfXg/QzCzB4EHAerqxvlFqL8XOppCD5pTrTDQ\nE6pZ0tLDc6wvXOy728JzV0voodN1NNyZ950Ks232d5//Z1VeAiv+Teihk54dSga9XeEuv3bF6fl4\nsvLPXrWjgVsiE0oik0IjUDtsu4a3Vw89AKwEcPdXzSwHqACODD/I3R8CHgJYtmzZmYlldMVi0NcF\nPSfCRTQ9M9SXn2oLd+eDF/O8ciiphRMtp+/Oc4rDnXXzxvC50vpQPdPeEBJBe+Pbp04+n9zSUK9f\nUBWSRkZWmGCtpC7c4WfmhPr7vlPhYl81P7qjNw3GEpF3LJFJYQ0w28ymA03Ax4F7zzjmAHAz8F0z\nmwfkAC0JjCkscrL5Mdj3crh4n2oNo149Ft2Jnzz/d5yLpYU7dI/BzmchLSOskFVUHS7mxbXhdcGk\ncMHPzAn1/rEB8IFwQc8pCe/lFI9cZSMikiAJSwru3m9mnwd+Rehu+h1332JmfwqsdfcngT8A/t7M\nfp9w+/xpd09sSeC5P4FXvx7usmuWwZTFoYHU0kMVyuAsl9kFkJkXkshAT7hQ55WH0bS5paHE0N4Q\nZsqsmBOSQXdbGD2blTd4EsKzet2IyDhhib4Gj7Zly5b52rVrL+zDrfvg61fAgo/AXd/QEociMmGY\n2Tp3X3a+4ybWVfGf/2soEdz0NSUEEZERTJwr48ENsPkncOVnQx2/iIi8zcRJCgdeDfX/1/77ZEci\nIpKyJs6EeFd+FhbfBzlF5z9WRGSCmjglBVBCEBE5j4mVFERE5JyUFEREZIiSgoiIDFFSEBGRIUoK\nIiIyRElBRESGKCmIiMgQJQURERmipCAiIkOUFEREZIiSgoiIDFFSEBGRIUoKIiIyRElBRESGKCmI\niMiQ8yYFM/sdMyuMXn/NzB43s6WJD01ERMZaPCWFP3L3TjO7FrgNeBj4ZmLDEhGRZIgnKQxEz+8H\nvunuPweyEheSiIgkSzxJocnM/g74KPC0mWXH+TkRERln4rm4fxT4FbDS3duAMuDLCY1KRESSIiOO\nY6YA/+TuPWZ2A3AZ8L2ERiUiIkkRT0nhp8CAmc0Cvg1MB36Q0KhERCQp4kkKMXfvBz4E/LW7/z6h\n9CAiIheZeJJCn5ndA3wKeCral5m4kEREJFniSQqfAa4C/pu77zWz6cAjiQ1LRESS4bxJwd23Al8C\nNpvZAqDR3f8s4ZGJiMiYO2/vo6jH0cPAPsCAWjO7391fSmxoIiIy1uLpkvqXwHvd/U0AM5sD/BC4\nPJGBiYjI2IunTSFzMCEAuPsO1NAsInJRiicprDWzb5vZDdHj74F18Xy5ma00szfNbJeZfeUsx3zU\nzLaa2RYz0/gHEZEkiqf66LPA54AvENoUXgK+cb4PmVl6dNytQCOwxsyejBquB4+ZDXwVuMbdW82s\n6p3/CiIiMlrOmxTcvQf4X9EDADP7EfCx83x0ObDL3fdEn3kUuAvYOuyYfw18w91bo5915B1FLyIi\no+pCZzu9Ko5jqoGGYduN0b7h5gBzzOxfzGyVma28wHhERGQUxFN9dKFshH0+ws+fDdwA1AAvm9mC\naDbW019k9iDwIEBdXd3oRyoiIsA5ksI5ltw04ut91AjUDtuuAQ6OcMwqd+8D9prZm4QksWb4Qe7+\nEPAQwLJly85MLCIiMkrOVVL4y3O8tz2O714DzI6mxWgCPg7ce8YxPwPuAb5rZhWE6qQ9cXy3iIgk\nwLmSwn3ufuadfdzcvd/MPk9YoCcd+I67bzGzPwXWuvuT0XvvNbOthGU/v+zuxy70Z4qIyLtj7iPX\nxpjZL4FS4AXgGeCVaArtpFq2bJmvXbs22WGIiIwrZrbO3Zed77izlhTc/XYzyyE0At8N/IWZHSAk\niGfc/cBoBSsiIqnhnL2P3L2bKAkARO0DtwNfN7PJ7r488SGKiMhYOe84BTP7vJmVALj7Xnf/3+5+\nJ3BtwqMTEZExFc/gtcmE+Y9+HM1lZADu3pvY0EREZKzFs8jO1whjB74NfBrYaWb/3cxmJjg2EREZ\nY3FNc+Ghi9Kh6NFP6JX0mJn9eQJjExGRMRbPymtfAO4HjgLfIowl6DOzNGAn8B8TG6KIiIyVeOY+\nqgA+5O77h+9095iZ3ZGYsEREJBniqT56Gjg+uGFmhWa2AsDdtyUqMBERGXvxJIVvAieGbXdF+0RE\n5CITT1IwHzYXhrvHSOyU2yIikiTxJIU9ZvYFM8uMHl9EM5mKiFyU4kkKvwdcTZj+uhFYQbTgjYiI\nXFziWaP5CGEtBBERucjFM04hB3gAuBTIGdzv7r+bwLhERCQJ4qk++j5h/qPbgBcJy2p2JjIoERFJ\njniSwix3/yOgy90fBt4PLExsWCIikgzxJIW+6LnNzBYAxUB9wiISEZGkiWe8wUNmVgp8DXgSKAD+\nKKFRiYhIUpyzpBBNetfh7q3u/pK7z3D3Knf/uzGKb9Ss3nOMP/75G5xtTWoRETlPUohGL39+jGJJ\nqN0tXTz86n62HOxIdigiIikrnjaFZ83sS2ZWa2Zlg4+ERzbKbl8wmYw04xebDiY7FBGRlBVPUvhd\n4HPAS8C66LE2kUElQml+FtfNruCpjc2qQhIROYt4luOcPsJjxlgEN9o+sGgqTW2nWH+gNdmhiIik\npHhGNH9qpP3u/r3RDyexbp0/iayMNH6xsZnLp427GjARkYSLp0vqFcNe5wA3A+uBcZcUCnMyuWlu\nFU+83sTJ3n6yM9Ipzc+iLC+TsoJsyvOzKM3LorwgPGdlxLWEtYjIRSOeCfH+3fBtMysmTH0xLt1/\ndT3bD3Xw0o6jdPcP0H6qj7M1MRRmZ1BWkEVZftZQwigryHpL8ijLz6YsL4vKwmxys9LH9pcRERll\nF7JYzklg9mgHMlaumlnOC1++cWi7fyBG+6k+jnf1cqyrl9bo+fgZj4Nt3bzR1MHxrl56B2IjfndB\ndgaVhdlUFmSH58HHsO2qwmzK8rPISFcpRERSTzxtCr8ABu+l04D5wI8TGdRYykhPo7wgm/KC7Lgy\nnbvT1TvA8RO9HD/Zy/GuHo6d6OXoiV5aOntoOdFDS2c32w918PLOHjq6+9/2HWZQnp9FxXmSR2Vh\nDkU5GZjZ6P/iIiIjiKek8BfDXvcD+929MUHxpDwzoyA7g4LsDOrK8857fHffwLBkMewxbHtPSxct\nnT0jlkCyMtKoLMhmUlE208rzqSvLo74ij5rSPIpzMynJzaSiIJu0NCUOEXn34kkKB4Bmd+8GMLNc\nM6t3930JjewikZOZTm1ZHrVl504g7k5Hdz8tnd0cGSF5HGrv5rW9x/nZhqa3tYFkZ6QxrTyPurJ8\n6svzmD+1iEunFjO9Il+N5SLyjsSTFH5CWI5z0EC074qRD5cLYWYU52ZSnJvJrKrCsx7X0z9Aw/FT\nNLWdorO7j9auXg4cP8m+Yyc5cOwkL+9soac/lDjS04za0lxmVBYwszKfGZUFzKjI55LJRRTnZY7V\nryYi40g8SSHD3XsHN9y918yyEhiTnEN2RjqzqgqYVVUw4vv9AzH2HO1iy8F2dh/pYs/RE+xp6eJf\ndh0dShYAU4tzmDWpkBkV+UMJY+7kQioKssfqVxGRFBRPUmgxszvd/UkAM7sLOJrYsORCZaSnMWdS\nIXMmvbW0EYs5TW2n2NVygu3NnWxr7mB3ywnW7jvOyd6BoeMmFWVz6dRiLp1axPwpRcybUkRdWZ7a\nLEQmiHiSwu8B/2hmX4+2G4ERRzlL6kpLs6G2jRvnVg3td3cOd/Swu+UE25o72Hqwgy0HO3hxRwsD\nsdB4kZeVztzJhcyLksTC6mLmTylSe4XIRcjinRzOzAqi4+Nen9nMVgJ/A6QD33L3PzvLcR8haqdw\n93NOtrds2TJfu3bczcc37nT3DbDjcCfbmzvZ2tzBtugx2MU2OyONS6YUMaMin9mTClhUU8LCmmKK\nctRWIZKKzGyduy8733HxjFP478Cfu3tbtF0K/IG7f+08n0sHvgHcSihdrDGzJ9196xnHFQJfAFaf\nLxYZOzmZ6VxWU8JlNSVD+9ydg+3dbGpoY+3+VrYf6mDVnmM88XrT0DEzKvNZXFPCZTXFLKwp4ZLJ\nheRnX8gYSRFJhnj+Wm939/88uOHurWb2PsLynOeyHNjl7nsAzOxR4C5g6xnH/b/AnwNfijtqSQoz\no7okl+qSXG5fOGVof2tXL5ua2tnU0MbGxnZe3nWUx6NEYQbTy/O5fFopy+pLWVBdzOyqQlU9iaSo\neJJCupllu3sPhHEKQDxdVKqBhmHbjcCK4QeY2RKg1t2fMjMlhXGqND+L98yp5D1zKoFQojjU0c3m\nxna2H+pkU2M7z247zE/WhTGPWelpzJ1cyILqMJ5iQXUx86YUkp2huaNEki2epPAI8LyZ/UO0/Rng\n4Tg+N1J3laEGjGj9578CPn3eLzJ7EHgQoK6uLo4fLclkZkwpzmVKcS7vvXQyEHo/7TvWxZaDHbxx\nsJ0tTR388o1D/PC1cN+QlZ7GvKlFLKktYXH0mFaepyk+RMZYXA3NUYPxLYQLfSswxd0/d57PXAX8\nibvfFm1/FcDd/0e0XQzsBk5EH5kMHAfuPFdjsxqaLx7uoZvs5sZ2NjS2seFAG5sa2znVF7rIluZl\nsmhYklhcW0JJnobIiFyIUWtojhwCYsBHgb3AT+P4zBpgtplNB5qAjwP3Dr7p7u1AxbCAXwC+dL7e\nR3LxMDNqSsM8ToNtFP0DMXYcPsGGhjY2NLSyoaGNF3e0DE3tMbMyn6V1pVw+LTxmVhZoDIXIKDpr\nUjCzOYQL+T3AMeBHhJLFjWf7zHDu3m9mnwd+ReiS+h1332JmfwqsHRwMJzJcRnoa86cWMX9qEfeu\nCFWFnd19bG5s5/WGNtbvb31L+0RRTgZLp5W+JVHkZKptQuRCnbX6yMxiwMvAA+6+K9q3J9nrM6v6\nSNydvUe7WLe/lfUHWlm3v5Udh0MtZE5mGtfMrOCK6WXMmVTA4tpSyvJV5SQyGtVHHyaUFH5jZs8A\njzJy47HImDKzMLlfZQG/s6wWgPaTfaw/0MqLO1p4fvthnt9+JDoWFlYXc/XMCq6oDyUJtUuInN15\nG5rNLB/4IKEa6SZCz6Mn3P3XiQ/v7VRSkHi0n+zjzcOdrN5zjBd3tLCxsY2+gfB/fXZVAStmlHH9\n7EqunlVBgQbXyQQQb0kh7mkuoi8tA34H+Ji73/Qu4rtgSgpyIbr7BtgYjcRes+84r+0NEwFmpBmX\nTyvl+micxfwpRWq4lotSQpJCKlBSkNHQ2x9j3f5Q3fTSjha2NncAUFGQxXWzK7ludgUrZpRTXZKb\n5EhFRoeSgsg7cKSzm1d2HuXFHS28vPMox7vCEiI1pbmsmF7OVTPLuXFuJeVab0LGKSUFkQsUiznb\nD3Wyeu8xVu05xmt7j9N6sg8zWFpXyi3zJnHLvCpmVRVoxLWMG0oKIqMkFnO2Nnfw3LbDPLftMG80\nhaqmKcU5XD+7kpULJ3PtrAoy0zXJn6QuJQWRBGluP8Vvtrfwyq4WXt5xlM6efopzM7ludgU3zK3i\nPXMqqSxUNZOkFiUFkTHQ0z/AKzuP8ss3DvHijhZaOnuAMDbijsumcOfiqUwpVmO1JJ+SgsgYG6xm\neuHNIzy77QgbG9oAmD+liKtnlnPHoqksqilWO4QkhZKCSJLtPdrF05ub+ZddR1m7v5Xe/hhzJxVy\ny/yqaIR1mRYbkjGjpCCSQjq7+/jFxmYeX9/I6w1tDMScopwMVi6YzAcWTeWqGeVkqKFaEkhJQSRF\nnejpZ9XuYzy9uZlfbz3MiZ5+yvOzuH3hZO64bCpX1JeRrlHVMsqUFETGge6+AV54s4VfbDrI89sO\n090Xo6Igm/deOonbLp3MVTPKVcUko0JJQWSc6erp5/ntR/jVG4f4zZtHONk7QGFOBrfOn8Rdi6u5\nZqaqmOTCKSmIjGPdfaGr6zNbDvGrLYfo7O6noiCL9y+cwp2Lq1laV6JeTPKOKCmIXCR6+kMV05Mb\nDvLctsP09MeoKc3lrsVTuXtJNbOqCpMdoowDSgoiF6HO7j5+veUwP994kFd2thBzWFBdxAcXV3Pn\noqlUFeUkO0RJUUoKIhe5I53dPLWxmZ9taGJTYztpBtfMquBDS6tZeekUcrO0VrWcpqQgMoHsOnKC\nn29o4onXm2hsPUVhdgZ3LJrKR5fVsLhW7Q+ipCAyIcVizmv7jvOTtY08vbmZU30DzK4q4O6l1dy9\npFrzME1gSgoiE1xndx//tKmZx9Y1snZ/K2kGt8ybxKevrufKGeVadnSCUVIQkSH7j3XxozUN/PC1\nA7Se7KO6JJcPLpnK3UtqmFVVkOzwZAwoKYjI23T3DfCrLYd4fH0TL0e9lxbVFHP3kmo+sGiqlhu9\niCkpiMg5Heno5smNB3l8fRNbmzvISDNumFvJ3UtquHleFTmZ6r10MVFSEJG4bT/UwRPrm/jZhiYO\nd/RQlJPBBxZN5f6r65kzSYPjLgZKCiLyjg3EnN/uPspP1zXyzJZDdPfFuHFuJR+7opYb5qr0MJ7F\nmxQyxiIYERkf0tOM62ZXct3sSlq7enlk1X4efnU/v3lzPQXZGdy1eCqfuqqeuZNVerhYqaQgIufU\nPxBj1Z7jPPF6E09tOkhPf4zl08v45JXTWLlgMpmauXVcUPWRiIy61q5efrKugUdWHeDA8ZNUFmZz\nz/I67l1ex+RizbuUypQURCRhYjHnxR0tfO/Vfbywo4U0M267dBKfvLKeK2eUaVqNFKQ2BRFJmLQ0\n48ZLqrjxkioOHDvJI6v38+O1DTy9+RCzqwr4+PI6PrBoClWFKj2MNyopiMio6O4b4BcbD/LIqv1s\njGZtvemSSTx4/QyuqC9V6SHJVH0kIkmz68gJnni9kR+sDtNqLKgu4t7l07hr8VTys1VBkQwpkRTM\nbCXwN0A68C13/7Mz3v8PwL8C+oEW4Hfdff+5vlNJQWT8ONU7wE/XN/LIqv1sP9Q51K31vhXTmD+1\nKNnhTShJTwpmlg7sAG4FGoE1wD3uvnXYMTcCq939pJl9FrjB3T92ru9VUhAZf9yd9Qfa+MHqA0Pd\nWi+fVsonr5zG7Qsnk52hQXGJlgpJ4SrgT9z9tmj7qwDu/j/OcvwS4Ovufs25vldJQWR8az/Zx2NR\n6WHv0S7K8rP46LJa7ltRR21ZXrLDu2ilQu+jaqBh2HYjsOIcxz8A/DKB8YhICijOy+SBa6fzmavr\n+e3uY3x/1T4eemk3f/fSbm6cW8Unr5zG9XMqSdd6D0mRyKQw0r/oiMUSM/sEsAx4z1nefxB4EKCu\nrm604hORJEpLM66dXcG1sys42HaKR187wA9ea+Az311DXVkeD14/g49cXqP5lsZY0quPzOwW4P8H\n3uPuR873vao+Erl49fbH+NWWQ3zrlb1sbGijoiBULd2zXFVL71YqtClkEBqabwaaCA3N97r7lmHH\nLAEeA1a6+854vldJQeTi5+68uucY//Av+3h+22EcuHFuFZ+4so73zKlS1dIFSHpSiIJ4H/DXhC6p\n33H3/2Zmfwqsdfcnzew5YCHQHH3kgLvfea7vVFIQmVgGq5Z+uKaBls4eqktyuXdFHR9dVktloVaK\ni1dKJIVEUFIQmZj6BmI8u/X/PD2sAAAKcUlEQVQwj6zaz293HyMz3bjt0sl84spprJiu+ZbOJxV6\nH4mIjJrM9DTet3AK71s4hd0tJ/jB6gM8tq6RpzY1M7uqgPtW1HH30hqKczOTHeq4ppKCiIxbQ/Mt\nrT7AxoY2cjPTuXtpNfdrIaC3UfWRiEwomxvbeWTVfn62oYme/hhXzSjn/qvruXlelRYCQklBRCao\n1q5eHl3TwCOr9tPUdory/CzuuGwK96yo45LJE3e+JSUFEZnQ+gdivPBmC09saOLZrYfp7Y+xYnoZ\nn766nlvnTyJjgpUelBRERCKtXb38aG0D3381lB6mFudw35XTuGd5HWX5WckOb0woKYiInGEg5jy3\n7TAP/3Yfv919jKyMNG6ZV8XtC6Zw87wq8rIu3g6Z6pIqInKG9LQwtuG2Syez43Anj6zaz9ObD/H0\n5kMUZmfwwSXV3LO8bkKv9aCSgohMaAMxZ82+4/x4TQNPbW6mtz/GotoS7l1eyx2XXTwrxan6SETk\nHWo72cvj65v44WsH2HnkBAXZGXxg0VQ+cnkNS+tKxvWoaSUFEZEL5O6s29/KD147wNObm+nuizGj\nIp8PX17Dh5ZWM6U4N9khvmNKCiIio6Czu49fbj7EY+saeW3fcczg2lkVfHRZLbddOpmsjPHRtVVJ\nQURklO0/1sVP1zfx03WNNLWdoqIgmw9fXs2t8yaxpK40paf0VlIQEUmQWMx5cWcLj7y6nxd3tNAf\ncyoKsvjAoql8eGkNl04tSrn2ByUFEZEx0H6qj5d2tPBPm5r55+1H6B2IMbuqgLuXVvPBxdVMLUmN\n9gclBRGRMdZ2spd/2tzM4+ubWLe/FTO4cno5dy+t5vYFkynMSd603koKIiJJtP9YF0+83sQTrzex\n/9hJsjPSuGFuJe+/bCo3X1I15uMflBRERFKAu7P+QBtPbmji6TcO0dLZQ3ZGGjfOreJ9l00ZswSh\npCAikmIGYs7afcd5enPzWxLEtbMqeO+lk7jpkkkJW3daSUFEJIUNJohfvnGIZ7cepqntFGawtK6U\nW+dP4tb5k5hZWTBqP09JQURknHB3tjZ38OzWwzy79TBbDnYAMKMyn1vnT+K98yexuPbdjYNQUhAR\nGaea2k7xXJQgVu05NjQO4o/umM9di6sv6Ds1dbaIyDhVXZLL/VfXc//V9bSf6uOFN4/w3LYjTC7K\nSfjPVlIQEUlhxbmZ3LW4+oJLCO/U+JjJSURExoSSgoiIDFFSEBGRIUoKIiIyRElBRESGKCmIiMgQ\nJQURERmipCAiIkPG3TQXZtYC7L/Aj1cAR0cxnERQjKNDMY6OVI8x1eOD1IlxmrtXnu+gcZcU3g0z\nWxvP3B/JpBhHh2IcHakeY6rHB+MjxuFUfSQiIkOUFEREZMhESwoPJTuAOCjG0aEYR0eqx5jq8cH4\niHHIhGpTEBGRc5toJQURETmHCZMUzGylmb1pZrvM7CvJjgfAzGrN7Ddmts3MtpjZF6P9ZWb2rJnt\njJ5Lkxxnupm9bmZPRdvTzWx1FN+PzCwryfGVmNljZrY9OpdXpeA5/P3o3/gNM/uhmeUk+zya2XfM\n7IiZvTFs34jnzYK/jf5+NpnZ0iTG+P9F/9abzOwJMysZ9t5XoxjfNLPbkhXjsPe+ZGZuZhXRdlLO\n4zsxIZKCmaUD3wBuB+YD95jZ/ORGBUA/8AfuPg+4EvhcFNdXgOfdfTbwfLSdTF8Etg3b/p/AX0Xx\ntQIPJCWq0/4GeMbdLwEWEWJNmXNoZtXAF4Bl7r4ASAc+TvLP43eBlWfsO9t5ux2YHT0eBL6ZxBif\nBRa4+2XADuCrANHfzseBS6PP/O/obz8ZMWJmtcCtwIFhu5N1HuM2IZICsBzY5e573L0XeBS4K8kx\n4e7N7r4+et1JuJhVE2J7ODrsYeCDyYkQzKwGeD/wrWjbgJuAx6JDkh1fEXA98G0Ad+919zZS6BxG\nMoBcM8sA8oBmknwe3f0l4PgZu8923u4CvufBKqDEzKYkI0Z3/7W790ebq4CaYTE+6u497r4X2EX4\n2x/zGCN/BfxHYHjDbVLO4zsxUZJCNdAwbLsx2pcyzKweWAKsBia5ezOExAFUJS8y/prwHzsWbZcD\nbcP+KJN9LmcALcA/RFVc3zKzfFLoHLp7E/AXhDvGZqAdWEdqncdBZztvqfo39LvAL6PXKROjmd0J\nNLn7xjPeSpkYz2aiJAUbYV/KdLsyswLgp8C/d/eOZMczyMzuAI64+7rhu0c4NJnnMgNYCnzT3ZcA\nXSS/uu0tonr5u4DpwFQgn1CNcKaU+T85glT7d8fM/pBQBfuPg7tGOGzMYzSzPOAPgf8y0tsj7Eup\nf/eJkhQagdph2zXAwSTF8hZmlklICP/o7o9Huw8PFimj5yNJCu8a4E4z20eocruJUHIoiapBIPnn\nshFodPfV0fZjhCSRKucQ4BZgr7u3uHsf8DhwNal1Hged7byl1N+Qmd0P3AHc56f71adKjDMJNwAb\no7+dGmC9mU0mdWI8q4mSFNYAs6PeHlmExqgnkxzTYP38t4Ft7v6/hr31JHB/9Pp+4OdjHRuAu3/V\n3WvcvZ5wzv7Z3e8DfgN8JNnxAbj7IaDBzOZGu24GtpIi5zByALjSzPKif/PBGFPmPA5ztvP2JPCp\nqPfMlUD7YDXTWDOzlcB/Au5095PD3noS+LiZZZvZdEJj7mtjHZ+7b3b3Knevj/52GoGl0f/VlDmP\nZ+XuE+IBvI/QU2E38IfJjieK6VpC0XETsCF6vI9Qb/88sDN6LkuBWG8AnopezyD8se0CfgJkJzm2\nxcDa6Dz+DChNtXMI/D/AduAN4PtAdrLPI/BDQhtHH+HC9cDZzhuh2uMb0d/PZkJPqmTFuItQLz/4\nN/N/hh3/h1GMbwK3JyvGM97fB1Qk8zy+k4dGNIuIyJCJUn0kIiJxUFIQEZEhSgoiIjJESUFERIYo\nKYiIyBAlBZGImQ2Y2YZhj1EbGW1m9SPNoimSajLOf4jIhHHK3RcnOwiRZFJJQeQ8zGyfmf1PM3st\nesyK9k8zs+ejefGfN7O6aP+kaJ7/jdHj6uir0s3s7y2sq/BrM8uNjv+CmW2NvufRJP2aIoCSgshw\nuWdUH31s2Hsd7r4c+Dph/iei19/zMK//PwJ/G+3/W+BFd19EmIdpS7R/NvANd78UaAM+HO3/CrAk\n+p7fS9QvJxIPjWgWiZjZCXcvGGH/PuAmd98TTWB4yN3LzewoMMXd+6L9ze5eYWYtQI279wz7jnrg\nWQ+L12Bm/wnIdPf/ambPACcIU3T8zN1PJPhXFTkrlRRE4uNneX22Y0bSM+z1AKfb9N5PmA/ncmDd\nsJlTRcackoJIfD427PnV6PVvCbPHAtwHvBK9fh74LAytb110ti81szSg1t1/Q1jMqAR4W2lFZKzo\njkTktFwz2zBs+xl3H+yWmm1mqwk3UvdE+74AfMfMvkxY/e0z0f4vAg+Z2QOEEsFnCbNojiQdeMTM\nigkzaP6Vh+VERZJCbQoi5xG1KSxz96PJjkUk0VR9JCIiQ1RSEBGRISopiIjIECUFEREZoqQgIiJD\nlBRERGSIkoKIiAxRUhARkSH/F5lahFum9x3aAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x173b74a2828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist.history\n",
    "x = np.arange(epochs)\n",
    "yloss = hist.history['loss']\n",
    "yacc = hist.history['acc']\n",
    "\n",
    "plt.plot(x,yloss)\n",
    "plt.plot(x,yacc)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy/Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluating the model\n",
    "This will give you the accuracy of the model, as evaluated on the testing set. Can you get something over 85%?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  0.98804\n",
      "Test Accuracy:  0.746\n",
      "Epochs: 150  Batch Size: 50  Learning Rate: 0.0001\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_train, y_train, verbose=0)\n",
    "print(\"Train Accuracy: \", score[1])\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test Accuracy: \", score[1])\n",
    "print(\"Epochs:\", epochs, \" Batch Size:\",batch_size, \" Learning Rate:\", eval(model.optimizer.lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.80473467105865482, 0.746]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the model architecture with one layer of length 100\n",
    "smodel = Sequential()\n",
    "smodel.add(Dense(512, activation='relu', input_dim=num_words))\n",
    "smodel.add(Dropout(0.3))\n",
    "smodel.add(Dense(num_classes, activation='softmax'))\n",
    "smodel.summary()\n",
    "\n",
    "# Compiling the model using categorical_crossentropy loss, and rmsprop optimizer.\n",
    "smodel.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Running and evaluating the model\n",
    "hist = smodel.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=10,\n",
    "          validation_data=(x_test, y_test), \n",
    "          verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sscore = smodel.evaluate(x_train, y_train, verbose=0)\n",
    "print(\"Train Accuracy: \", sscore[1])\n",
    "sscore = smodel.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test Accuracy: \", sscore[1])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
